{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb67b19-c108-4efa-8635-b62f59767b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "import torch.nn as nn\n",
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7435325f-1147-4a66-a775-1b2cc80216f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "stemmer=SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40faa6d-987e-499f-b664-b6e364e5b3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('//content//IMDB Dataset.csv')\n",
    "reviews=[]\n",
    "sentiments=[]\n",
    "for i in range(len(df['review'])):\n",
    "    reviews.append(df[\"review\"][i])\n",
    "    \n",
    "sentiments = df['sentiment'].map({'positive': 1, 'negative': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048e43d0-c998-4e41-b9c8-da484a846218",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "def preprocess(texts, max_length=300):\n",
    "    processed = []\n",
    "    for text in texts:\n",
    "        text = re.sub(r'<br\\\\s*/?>', ' ', text)\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        tokens = [stemmer.stem(w) for w in tokens if w not in stop_words]\n",
    "        processed.append(tokens[:max_length])\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0456f3f2-66d1-40f7-9f18-ea7c7e57d0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_reviews = preprocess(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc1a6fd-736b-4612-838d-4d0dc73512bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {'<unk>': 0}\n",
    "for tokens in tokenized_reviews:\n",
    "    for token in tokens:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary[token] = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d409bf2-bee0-49af-8197-ad5828d2ef26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_indices(tokens, vocab):\n",
    "    return [vocab.get(token, vocab['<unk>']) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d86ad1-e553-4fd7-8c58-3ad80d4a2ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_tensors = [torch.tensor(text_to_indices(r, vocabulary), dtype=torch.long) for r in tokenized_reviews]\n",
    "padded_reviews = pad_sequence(review_tensors, batch_first=True, padding_value=0)\n",
    "review_lengths = torch.tensor([len(r) for r in review_tensors])\n",
    "labels = torch.tensor(sentiments, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037c9255-0010-4cae-9084-5950823f4737",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, data, labels, lengths):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.lengths = lengths\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx], self.lengths[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1117a76-1dd3-4249-838c-67d11afbdc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ReviewDataset(padded_reviews, labels, review_lengths)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1777fb09-712e-4982-84e9-9164ac3e8f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=50, hidden_dim=100):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        embedded = self.embedding(x)\n",
    "        packed = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, (hn, _) = self.lstm(packed)\n",
    "        hn = self.dropout(hn[-1])\n",
    "        return self.fc(hn).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1446386d-bdc1-4e74-bddd-5ed74e34af7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = LSTMModel(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b7b46f-be76-4692-8558-d019a32e7bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMModel(len(vocabulary)).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4402dfc-c8db-4262-b5b3-99d2102f15a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    for xb, yb, lens in train_dl:\n",
    "        xb, yb, lens = xb.to(device), yb.to(device), lens.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(xb, lens)\n",
    "        loss = criterion(out, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        preds = (torch.sigmoid(out) > 0.5).float()\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {total_loss:.4f}, Accuracy = {correct/total:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a8fde0-6c20-4f01-a78a-924fafb18d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "   # Validation\n",
    "model.eval()\n",
    "val_loss, val_correct, val_total = 0, 0, 0\n",
    "with torch.no_grad():\n",
    "    for xb, yb, lens in val_dl:\n",
    "        xb, yb, lens = xb.to(device), yb.to(device), lens.to(device)\n",
    "        out = model(xb, lens)\n",
    "        val_loss += criterion(out, yb).item()\n",
    "        preds = (torch.sigmoid(out) > 0.5).float()\n",
    "        val_correct += (preds == yb).sum().item()\n",
    "        val_total += yb.size(0)\n",
    "print(f\"         Val Loss = {val_loss:.4f}, Accuracy = {val_correct/val_total:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae00f98-8edf-4023-853d-f5708dcc9d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, model, vocab, device, max_len=300):\n",
    "    model.eval()\n",
    "    text = re.sub(r'<br\\\\s*/?>', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [w for w in tokens if w not in stop_words]\n",
    "    tokens = tokens[:max_len]\n",
    "\n",
    "    indices = [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
    "    tensor = torch.tensor(indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    length = torch.tensor([len(indices)]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(tensor, length)\n",
    "        prob = torch.sigmoid(output).item()\n",
    "        prediction = \"Positive\" if prob > 0.5 else \"Negative\"\n",
    "        print(f\"Prediction: {prediction} ({prob:.4f})\")\n",
    "\n",
    "string=str(input(\"Enter review:\"))\n",
    "predict_sentiment(string, model, vocabulary, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75327b90-8479-4213-9545-f514f3ec2d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Main Problems Identified\n",
    "1. Model Uses Only the Final Hidden State\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "output = self.fc(final_hidden_state.squeeze(0))\n",
    "You're feeding only the final hidden state into the output layer. This can work, but:\n",
    "\n",
    "If the input is padded, the final hidden state may correspond to padding.\n",
    "\n",
    "It ignores rich intermediate information from the sequence.\n",
    "\n",
    "âœ… Fix: Use intermediate_hidden_states[:, -1, :] to take the last non-padded output instead. Or better: use attention or mean pooling over outputs.\n",
    "\n",
    "2. Padding Not Masked in LSTM\n",
    "The LSTM doesn't know which parts are padding. It treats all time steps equally.\n",
    "\n",
    "âœ… Fix: Use pack_padded_sequence:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# In forward:\n",
    "lengths = (x != 0).sum(dim=1)  # assuming 0 is the padding\n",
    "embedded = self.embedding(x)\n",
    "packed = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "_, (final_hidden_state, _) = self.lstm(packed)\n",
    "output = self.fc(final_hidden_state.squeeze(0))\n",
    "3. Sentiment Tensor Shape is Wrong\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "sentiments.append([data[\"sentiment\"][i]])\n",
    "You're wrapping each label in an extra list, making shape [N, 1]. Not wrong per se, but awkward and may cause shape mismatches.\n",
    "\n",
    "âœ… Fix: Just append data[\"sentiment\"][i] directly.\n",
    "\n",
    "4. No Validation Split or Metric\n",
    "You are training and printing accuracy on the training set only, so:\n",
    "\n",
    "You're not checking generalization.\n",
    "\n",
    "The model may overfit or be undertrained and you wouldn't know.\n",
    "\n",
    "âœ… Fix:\n",
    "Split dataset into train/test using train_test_split and check validation accuracy separately.\n",
    "\n",
    "5. No Token Limit or Truncation\n",
    "Some reviews may be very long, and some very short:\n",
    "\n",
    "This causes inefficient padding (long tail of very long sequences).\n",
    "\n",
    "Your model has to learn irrelevant zero-padded info.\n",
    "\n",
    "âœ… Fix: Truncate or limit token count, e.g.,:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "MAX_LEN = 300\n",
    "for sentence in reviews_vectors:\n",
    "    reviews_vectors.append(sentence[:MAX_LEN])\n",
    "6. Poor Tokenization and No Cleaning\n",
    "You're using basic word_tokenize and just lowercasing:\n",
    "\n",
    "No lemmatization/stemming (Snowball was imported but unused).\n",
    "\n",
    "No stopword removal.\n",
    "\n",
    "re.sub is minimal (e.g., punctuation, HTML tags, etc.)\n",
    "\n",
    "âœ… Fix: Improve preprocessing â€” e.g., remove stopwords, lemmatize, etc.\n",
    "\n",
    "7. Learning Rate Might Be Too Low\n",
    "You're using lr=0.001 with Adam. This is okay, but if training loss doesn't decrease, try 0.0005 or 0.0001.\n",
    "\n",
    "8. Batch Size\n",
    "You're using batch_size=64 â€” this is generally fine, but with large padded sequences, it can become memory-heavy.\n",
    "\n",
    "âœ… In Summary â€” Suggestions\n",
    "Use pack_padded_sequence with real lengths.\n",
    "\n",
    "Track validation accuracy and loss separately.\n",
    "\n",
    "Improve tokenization (stopword removal, lemmatization).\n",
    "\n",
    "Truncate long reviews.\n",
    "\n",
    "Consider using all hidden outputs (mean-pooling, attention, etc.).\n",
    "\n",
    "Add dropout or regularization if model overfits.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
