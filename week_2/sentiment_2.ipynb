{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb67b19-c108-4efa-8635-b62f59767b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "import torch.nn as nn\n",
    "import nltk\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7435325f-1147-4a66-a775-1b2cc80216f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "stemmer=SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40faa6d-987e-499f-b664-b6e364e5b3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('//content//IMDB Dataset.csv')\n",
    "reviews=[]\n",
    "sentiments=[]\n",
    "for i in range(len(df['review'])):\n",
    "    reviews.append(df[\"review\"][i])\n",
    "    \n",
    "sentiments = df['sentiment'].map({'positive': 1, 'negative': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048e43d0-c998-4e41-b9c8-da484a846218",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "def preprocess(texts, max_length=300):\n",
    "    processed = []\n",
    "    for text in texts:\n",
    "        text = re.sub(r'<br\\\\s*/?>', ' ', text)\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        tokens = [stemmer.stem(w) for w in tokens if w not in stop_words]\n",
    "        processed.append(tokens[:max_length])\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0456f3f2-66d1-40f7-9f18-ea7c7e57d0e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_reviews = preprocess(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc1a6fd-736b-4612-838d-4d0dc73512bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = {'<unk>': 0}\n",
    "for tokens in tokenized_reviews:\n",
    "    for token in tokens:\n",
    "        if token not in vocabulary:\n",
    "            vocabulary[token] = len(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d409bf2-bee0-49af-8197-ad5828d2ef26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_indices(tokens, vocab):\n",
    "    return [vocab.get(token, vocab['<unk>']) for token in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d86ad1-e553-4fd7-8c58-3ad80d4a2ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "review_tensors = [torch.tensor(text_to_indices(r, vocabulary), dtype=torch.long) for r in tokenized_reviews]\n",
    "padded_reviews = pad_sequence(review_tensors, batch_first=True, padding_value=0)\n",
    "review_lengths = torch.tensor([len(r) for r in review_tensors])\n",
    "labels = torch.tensor(sentiments, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037c9255-0010-4cae-9084-5950823f4737",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "    def __init__(self, data, labels, lengths):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.lengths = lengths\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx], self.lengths[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1117a76-1dd3-4249-838c-67d11afbdc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ReviewDataset(padded_reviews, labels, review_lengths)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1777fb09-712e-4982-84e9-9164ac3e8f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim=50, hidden_dim=100):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        embedded = self.embedding(x)\n",
    "        packed = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        _, (hn, _) = self.lstm(packed)\n",
    "        hn = self.dropout(hn[-1])\n",
    "        return self.fc(hn).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1446386d-bdc1-4e74-bddd-5ed74e34af7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = LSTMModel(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b7b46f-be76-4692-8558-d019a32e7bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LSTMModel(len(vocabulary)).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4402dfc-c8db-4262-b5b3-99d2102f15a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0, 0, 0\n",
    "    for xb, yb, lens in train_dl:\n",
    "        xb, yb, lens = xb.to(device), yb.to(device), lens.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(xb, lens)\n",
    "        loss = criterion(out, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        preds = (torch.sigmoid(out) > 0.5).float()\n",
    "        correct += (preds == yb).sum().item()\n",
    "        total += yb.size(0)\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {total_loss:.4f}, Accuracy = {correct/total:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a8fde0-6c20-4f01-a78a-924fafb18d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "   # Validation\n",
    "model.eval()\n",
    "val_loss, val_correct, val_total = 0, 0, 0\n",
    "with torch.no_grad():\n",
    "    for xb, yb, lens in val_dl:\n",
    "        xb, yb, lens = xb.to(device), yb.to(device), lens.to(device)\n",
    "        out = model(xb, lens)\n",
    "        val_loss += criterion(out, yb).item()\n",
    "        preds = (torch.sigmoid(out) > 0.5).float()\n",
    "        val_correct += (preds == yb).sum().item()\n",
    "        val_total += yb.size(0)\n",
    "print(f\"         Val Loss = {val_loss:.4f}, Accuracy = {val_correct/val_total:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae00f98-8edf-4023-853d-f5708dcc9d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(text, model, vocab, device, max_len=300):\n",
    "    model.eval()\n",
    "    text = re.sub(r'<br\\\\s*/?>', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [w for w in tokens if w not in stop_words]\n",
    "    tokens = tokens[:max_len]\n",
    "\n",
    "    indices = [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
    "    tensor = torch.tensor(indices, dtype=torch.long).unsqueeze(0).to(device)\n",
    "    length = torch.tensor([len(indices)]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(tensor, length)\n",
    "        prob = torch.sigmoid(output).item()\n",
    "        prediction = \"Positive\" if prob > 0.5 else \"Negative\"\n",
    "        print(f\"Prediction: {prediction} ({prob:.4f})\")\n",
    "\n",
    "string=str(input(\"Enter review:\"))\n",
    "predict_sentiment(string, model, vocabulary, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75327b90-8479-4213-9545-f514f3ec2d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Main Problems Identified\n",
    "1. Model Uses Only the Final Hidden State\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "output = self.fc(final_hidden_state.squeeze(0))\n",
    "You're feeding only the final hidden state into the output layer. This can work, but:\n",
    "\n",
    "If the input is padded, the final hidden state may correspond to padding.\n",
    "\n",
    "It ignores rich intermediate information from the sequence.\n",
    "\n",
    "✅ Fix: Use intermediate_hidden_states[:, -1, :] to take the last non-padded output instead. Or better: use attention or mean pooling over outputs.\n",
    "\n",
    "2. Padding Not Masked in LSTM\n",
    "The LSTM doesn't know which parts are padding. It treats all time steps equally.\n",
    "\n",
    "✅ Fix: Use pack_padded_sequence:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# In forward:\n",
    "lengths = (x != 0).sum(dim=1)  # assuming 0 is the padding\n",
    "embedded = self.embedding(x)\n",
    "packed = pack_padded_sequence(embedded, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "_, (final_hidden_state, _) = self.lstm(packed)\n",
    "output = self.fc(final_hidden_state.squeeze(0))\n",
    "3. Sentiment Tensor Shape is Wrong\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "sentiments.append([data[\"sentiment\"][i]])\n",
    "You're wrapping each label in an extra list, making shape [N, 1]. Not wrong per se, but awkward and may cause shape mismatches.\n",
    "\n",
    "✅ Fix: Just append data[\"sentiment\"][i] directly.\n",
    "\n",
    "4. No Validation Split or Metric\n",
    "You are training and printing accuracy on the training set only, so:\n",
    "\n",
    "You're not checking generalization.\n",
    "\n",
    "The model may overfit or be undertrained and you wouldn't know.\n",
    "\n",
    "✅ Fix:\n",
    "Split dataset into train/test using train_test_split and check validation accuracy separately.\n",
    "\n",
    "5. No Token Limit or Truncation\n",
    "Some reviews may be very long, and some very short:\n",
    "\n",
    "This causes inefficient padding (long tail of very long sequences).\n",
    "\n",
    "Your model has to learn irrelevant zero-padded info.\n",
    "\n",
    "✅ Fix: Truncate or limit token count, e.g.,:\n",
    "\n",
    "python\n",
    "Copy\n",
    "Edit\n",
    "MAX_LEN = 300\n",
    "for sentence in reviews_vectors:\n",
    "    reviews_vectors.append(sentence[:MAX_LEN])\n",
    "6. Poor Tokenization and No Cleaning\n",
    "You're using basic word_tokenize and just lowercasing:\n",
    "\n",
    "No lemmatization/stemming (Snowball was imported but unused).\n",
    "\n",
    "No stopword removal.\n",
    "\n",
    "re.sub is minimal (e.g., punctuation, HTML tags, etc.)\n",
    "\n",
    "✅ Fix: Improve preprocessing — e.g., remove stopwords, lemmatize, etc.\n",
    "\n",
    "7. Learning Rate Might Be Too Low\n",
    "You're using lr=0.001 with Adam. This is okay, but if training loss doesn't decrease, try 0.0005 or 0.0001.\n",
    "\n",
    "8. Batch Size\n",
    "You're using batch_size=64 — this is generally fine, but with large padded sequences, it can become memory-heavy.\n",
    "\n",
    "✅ In Summary — Suggestions\n",
    "Use pack_padded_sequence with real lengths.\n",
    "\n",
    "Track validation accuracy and loss separately.\n",
    "\n",
    "Improve tokenization (stopword removal, lemmatization).\n",
    "\n",
    "Truncate long reviews.\n",
    "\n",
    "Consider using all hidden outputs (mean-pooling, attention, etc.).\n",
    "\n",
    "Add dropout or regularization if model overfits.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
